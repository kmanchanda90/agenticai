{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## LangSmith Tracking and tracing\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001CFA9C54980> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CFA9E5B4D0> root_client=<openai.OpenAI object at 0x000001CFA9D21850> root_async_client=<openai.AsyncOpenAI object at 0x000001CFA9E59760> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Agentic AI** refers to artificial intelligence systems designed with a degree of autonomy and decision-making capability that allows them to act independently to achieve specific goals or objectives. The term \"agentic\" is derived from \"agency,\" which in this context implies that the AI possesses the ability to perceive its environment, make decisions based on that perception, and take actions to influence outcomes without continuous human intervention.\\n\\n### Key Characteristics of Agentic AI:\\n\\n1. **Autonomy:** Agentic AI can operate independently, making decisions without needing step-by-step instructions from humans for every task.\\n\\n2. **Goal-Oriented Behavior:** These AI systems are programmed or trained to achieve specific objectives. They can prioritize actions that align with these goals.\\n\\n3. **Perception and Interaction:** Agentic AI can perceive its environment through sensors or data inputs and interact with that environment, whether physically (as in robots) or digitally (as in software agents).\\n\\n4. **Learning and Adaptation:** Many agentic AI systems incorporate machine learning, enabling them to improve their performance over time based on experiences or new data.\\n\\n### Examples of Agentic AI:\\n\\n- **Autonomous Vehicles:** Self-driving cars that navigate roads, make real-time decisions to avoid obstacles, and comply with traffic laws without human drivers.\\n\\n- **Personal Assistants:** AI like Siri or Alexa that can manage schedules, control smart home devices, and provide information based on user preferences and commands.\\n\\n- **Robotic Process Automation (RPA):** Software robots that automate repetitive tasks in business processes, such as data entry or transaction processing.\\n\\n- **Autonomous Drones:** Drones used for delivery, surveillance, or agricultural monitoring that can navigate and perform tasks with minimal human oversight.\\n\\n### Implications and Considerations:\\n\\nWhile agentic AI offers numerous benefits, including increased efficiency and the ability to perform complex tasks, it also raises important ethical and practical considerations:\\n\\n- **Accountability:** Determining who is responsible for the actions of an autonomous AI system.\\n\\n- **Ethical Decision-Making:** Ensuring that AI systems make decisions that align with human values and ethical standards.\\n\\n- **Safety and Security:** Protecting AI systems from errors, biases, or malicious attacks that could lead to unintended consequences.\\n\\n- **Job Displacement:** Addressing the impact of automation on employment and the workforce.\\n\\n### Future of Agentic AI:\\n\\nThe development of agentic AI continues to advance, with ongoing research focused on enhancing autonomy, improving decision-making capabilities, and ensuring ethical alignment. As these systems become more integrated into various aspects of society, it will be crucial to establish robust frameworks for governance, accountability, and ethical standards to maximize benefits while mitigating potential risks.\\n\\n---\\n\\nIn summary, agentic AI encompasses autonomous systems capable of making decisions and taking actions to achieve defined goals, operating with minimal human intervention. Its applications are diverse, spanning from everyday consumer devices to complex industrial and societal systems, each presenting unique opportunities and challenges.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 983, 'prompt_tokens': 13, 'total_tokens': 996, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_5c373483f1', 'finish_reason': 'stop', 'logprobs': None} id='run-f631acd1-44b8-478b-a6b9-cac0ca5d4a9e-0' usage_metadata={'input_tokens': 13, 'output_tokens': 983, 'total_tokens': 996, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to artificial intelligence systems designed with a degree of autonomy and decision-making capability that allows them to act independently to achieve specific goals or objectives. The term \"agentic\" is derived from \"agency,\" which in this context implies that the AI possesses the ability to perceive its environment, make decisions based on that perception, and take actions to influence outcomes without continuous human intervention.\n",
      "\n",
      "### Key Characteristics of Agentic AI:\n",
      "\n",
      "1. **Autonomy:** Agentic AI can operate independently, making decisions without needing step-by-step instructions from humans for every task.\n",
      "\n",
      "2. **Goal-Oriented Behavior:** These AI systems are programmed or trained to achieve specific objectives. They can prioritize actions that align with these goals.\n",
      "\n",
      "3. **Perception and Interaction:** Agentic AI can perceive its environment through sensors or data inputs and interact with that environment, whether physically (as in robots) or digitally (as in software agents).\n",
      "\n",
      "4. **Learning and Adaptation:** Many agentic AI systems incorporate machine learning, enabling them to improve their performance over time based on experiences or new data.\n",
      "\n",
      "### Examples of Agentic AI:\n",
      "\n",
      "- **Autonomous Vehicles:** Self-driving cars that navigate roads, make real-time decisions to avoid obstacles, and comply with traffic laws without human drivers.\n",
      "\n",
      "- **Personal Assistants:** AI like Siri or Alexa that can manage schedules, control smart home devices, and provide information based on user preferences and commands.\n",
      "\n",
      "- **Robotic Process Automation (RPA):** Software robots that automate repetitive tasks in business processes, such as data entry or transaction processing.\n",
      "\n",
      "- **Autonomous Drones:** Drones used for delivery, surveillance, or agricultural monitoring that can navigate and perform tasks with minimal human oversight.\n",
      "\n",
      "### Implications and Considerations:\n",
      "\n",
      "While agentic AI offers numerous benefits, including increased efficiency and the ability to perform complex tasks, it also raises important ethical and practical considerations:\n",
      "\n",
      "- **Accountability:** Determining who is responsible for the actions of an autonomous AI system.\n",
      "\n",
      "- **Ethical Decision-Making:** Ensuring that AI systems make decisions that align with human values and ethical standards.\n",
      "\n",
      "- **Safety and Security:** Protecting AI systems from errors, biases, or malicious attacks that could lead to unintended consequences.\n",
      "\n",
      "- **Job Displacement:** Addressing the impact of automation on employment and the workforce.\n",
      "\n",
      "### Future of Agentic AI:\n",
      "\n",
      "The development of agentic AI continues to advance, with ongoing research focused on enhancing autonomy, improving decision-making capabilities, and ensuring ethical alignment. As these systems become more integrated into various aspects of society, it will be crucial to establish robust frameworks for governance, accountability, and ethical standards to maximize benefits while mitigating potential risks.\n",
      "\n",
      "---\n",
      "\n",
      "In summary, agentic AI encompasses autonomous systems capable of making decisions and taking actions to achieve defined goals, operating with minimal human intervention. Its applications are diverse, spanning from everyday consumer devices to complex industrial and societal systems, each presenting unique opportunities and challenges.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide an answer based on the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide an answer based on the question.\"),\n",
    "        (\"user\",\"{input}\"),\n",
    "\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"LangSmith is a sophisticated tool designed to enhance the development and deployment of applications that leverage language models such as those offered by OpenAI or Hugging Face. It provides a platform for developers to effectively manage, optimize, and troubleshoot their language model-based applications.\\n\\nKey features of LangSmith include:\\n\\n1. **Evaluation and Testing**: LangSmith allows developers to evaluate the performance of their language models in different scenarios. This includes testing for accuracy, relevance, and the handling of edge cases that might occur during real-world usage.\\n\\n2. **Debugging and Monitoring**: Developers can use LangSmith to monitor their applications actively. It facilitates the identification of issues like incorrect model behavior or unexpected results, thereby allowing for quicker debugging and problem resolution.\\n\\n3. **Performance Optimization**: The tool aids in optimizing the application’s performance by providing insights into how language models are used and where potential inefficiencies lie. This can include suggesting improvements or optimizations in model handling and integration techniques.\\n\\n4. **User Feedback Integration**: LangSmith can integrate user feedback to refine and improve model outputs. Feedback loops ensure that the model's responses are aligned closely with user expectations and application goals.\\n\\n5. **Deployment Oversight**: It offers features for managing the deployment of applications, ensuring that models are updated and deployed effectively without downtime or significant disruption to service.\\n\\nLangSmith serves as a valuable asset for developers looking to harness the full potential of language models, providing them with the tools needed to create robust, efficient, and user-friendly applications.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 304, 'prompt_tokens': 34, 'total_tokens': 338, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'stop', 'logprobs': None} id='run-e53255d0-5db7-46e6-94b2-c87bac2e7991-0' usage_metadata={'input_tokens': 34, 'output_tokens': 304, 'total_tokens': 338, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a tool developed by LangChain to help developers build, observe, and evaluate applications powered by language models. It assists in addressing common challenges faced when working with these models, such as ensuring reliability and cost-efficiency. LangSmith offers various features, including:\n",
      "\n",
      "1. **Tracing and Instrumentation**: It provides detailed logging of interactions with language models, helping developers understand model behavior and track application performance.\n",
      "\n",
      "2. **Evaluation**: LangSmith includes tools for assessing the quality of outputs, enabling developers to systematically evaluate and compare different versions of models or configurations.\n",
      "\n",
      "3. **Adjustment and Tuning**: By offering insights into performance and areas for improvement, LangSmith helps developers fine-tune prompts, models, and workflows to enhance application quality and efficiency.\n",
      "\n",
      "4. **Integration**: It integrates well with the broader ecosystem of tools and libraries surrounding language models, especially those from the LangChain framework, allowing seamless adoption into existing workflows.\n",
      "\n",
      "Overall, LangSmith is designed to streamline the development and maintenance of language model-driven applications, making it easier to build robust, effective, and scalable language technology solutions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'LangSmith', 'description': 'LangSmith is a platform under the LangChain umbrella designed to assist developers in building and deploying applications using language models. It provides tools that help in evaluating, testing, debugging, and monitoring language model applications, ultimately making them more reliable and easier to manage.', 'features': ['Evaluation: Tools for assessing the performance of language model applications.', 'Testing: Frameworks to ensure that applications are functioning as expected.', 'Debugging: Resources to help identify and fix issues in language model implementations.', 'Monitoring: Capabilities to track application performance and logs for insights and improvements.'], 'targetAudience': 'Developers, data scientists, and engineers working with language model-based applications.'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"query\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1cfb620ae40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader =  WebBaseLoader(\"https://python.langchain.com/docs/tutorials/llm_chain/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates\\nIn this quickstart we\\'ll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it\\'s just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you\\'ll have a high level overview of:\\n\\n\\nUsing language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet\\'s dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\\n\\nSelect chat model:Groq▾GroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage\\nAIMessage(content=\\'Ciao!\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 3, \\'prompt_tokens\\': 20, \\'total_tokens\\': 23, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-32654a56-627c-40e1-a141-ad9350bbfd3e-0\\', usage_metadata={\\'input_tokens\\': 20, \\'output_tokens\\': 3, \\'total_tokens\\': 23, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\ntipIf we\\'ve enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:\\n\\nlanguage: The language to translate text into\\ntext: The text to translate\\n\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])\\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\\nThat\\'s it! In this tutorial you\\'ve learned how to create your first simple LLM application. You\\'ve learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we\\'ve got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we\\'ve got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\\n\\nChat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document=loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you'll have a high level overview of:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"Using language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet's dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='PipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Or, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Select chat model:Groq▾GroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\\ntipIf we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='for token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='language: The language to translate text into\\ntext: The text to translate'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='from langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"Conclusion\\u200b\\nThat's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we've got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Chat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(document)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1cfe237f6e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(documents,embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Conclusion\\u200b\\nThat's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we've got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"This is a relatively simple - \"\n",
    "\n",
    "result=vectorstore.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "\n",
    "Answer the following question based only on the provided context: \n",
    "< context> \n",
    "{context} \n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n\\nAnswer the following question based only on the provided context: \\n< context> \\n{context} \\n</context>\\n\\n\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n\\nAnswer the following question based only on the provided context: \\n< context> \\n{context} \\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CFB67C5AC0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CFB67C7500>, root_client=<openai.OpenAI object at 0x000001CFA9E83E30>, root_async_client=<openai.AsyncOpenAI object at 0x000001CFB67C5AF0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8a8c5e7b-7bee-403e-9023-fab598131565', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b'),\n",
       " Document(id='2f96fd52-d430-4b89-8b4c-07a5ca796911', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Select chat model:Groq▾GroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage'),\n",
       " Document(id='fbde0621-1981-454e-a5c0-fb597e61d323', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='for token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:'),\n",
       " Document(id='173df66e-be2e-49bf-8d2e-ba7672f073aa', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"Note that ChatModels receive message objects as input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, here are some key points about the functionality and features:\\n\\n1. **Message Objects**: Chat models operate with message objects that include text, roles, and metadata such as tool calls and token usage.\\n\\n2. **Invocation Formats**: There are multiple ways to input messages into a chat model: directly as a string, as a structured dictionary with role and content, or using specific message objects like `HumanMessage`.\\n\\n3. **Streaming**: Chat models support streaming individual tokens, useful for processing or displaying ongoing chat interactions in real-time.\\n\\n4. **Prompt Templates**: These allow for the creation of reusable chat prompts with placeholders for input and partial variables, allowing customization and flexibility for generating prompts from various structures.\\n\\nThis context outlines how chat models can be used, adapted, and extended for complex interactions by integrating structured message formats and streaming capabilities.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke({\n",
    "    \"input\":\"Note that ChatModels receive message objects as input\",\n",
    "    \"context\":[Document(page_content=\"\"\"Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nPrompt templates are a powerful tool for creating reusable chat prompts. They allow you to define a template with placeholders for input variables and partial variables. Prompt templates can be used to create chat prompts from messages, templates, or templates with partial variables. They can also be used to create chat prompts from templates with partial variables and input variables.\\n\"\"\")]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001CFE237F6E0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n\\nAnswer the following question based only on the provided context: \\n< context> \\n{context} \\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CFB67C5AC0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CFB67C7500>, root_client=<openai.OpenAI object at 0x000001CFA9E83E30>, root_async_client=<openai.AsyncOpenAI object at 0x000001CFB67C5AF0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retrieval_chain.invoke({\n",
    "    \"input\":\"Note that ChatModels receive message objects as input\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, how are message objects used with chat models in LangChain, and what additional functionality do they provide beyond just conveying text?\\n\\nMessage objects in LangChain are used to interact with chat models, providing more than just text content. They convey conversational roles (such as user or system messages) and hold important data, such as tool calls and token usage counts. This structure allows for more complex interactions and functionality, such as streaming individual tokens or using async modes, and it also helps track detailed information like token usage and model parameters through tools like LangSmith.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x182c109b770>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 100\n",
      "Created a chunk of size 347, which is longer than the specified 100\n",
      "Created a chunk of size 668, which is longer than the specified 100\n",
      "Created a chunk of size 982, which is longer than the specified 100\n",
      "Created a chunk of size 789, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='…'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='We have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='To such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter=CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=100,chunk_overlap=20)\n",
    "text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 100\n",
      "Created a chunk of size 347, which is longer than the specified 100\n",
      "Created a chunk of size 668, which is longer than the specified 100\n",
      "Created a chunk of size 982, which is longer than the specified 100\n",
      "Created a chunk of size 789, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'\n",
      "page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.'\n"
     ]
    }
   ],
   "source": [
    "speech=\"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech=f.read()\n",
    "\n",
    "\n",
    "text_splitter=CharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "text=text_splitter.create_documents([speech])\n",
    "print(text[0])\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings1=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x182c4661310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore1=FAISS.from_documents(text_documents,embeddings1)\n",
    "vectorstore1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"It is a distressing and oppressive duty, gentlemen of the Congress,\"\n",
    "\n",
    "result1=vectorstore1.similarity_search(query)\n",
    "result1[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n\\nAnswer the following question based only on the provided context: \\n< context> \\n{context} \\n</context>\\n\\n\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt1=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "\n",
    "Answer the following question based only on the provided context: \n",
    "< context> \n",
    "{context} \n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm1=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n\\nAnswer the following question based only on the provided context: \\n< context> \\n{context} \\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000182EFC3A2A0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000182EFC3FF50>, root_client=<openai.OpenAI object at 0x00000182EFCFFD70>, root_async_client=<openai.AsyncOpenAI object at 0x00000182EF9D1640>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm1,prompt1)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5893874e-53f9-4ce8-8a02-49460b3802a8', metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore1.similarity_search(\"Just because we fight without rancor and without selfish object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000182C4661310>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n\\nAnswer the following question based only on the provided context: \\n< context> \\n{context} \\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000182EFC3A2A0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000182EFC3FF50>, root_client=<openai.OpenAI object at 0x00000182EFCFFD70>, root_async_client=<openai.AsyncOpenAI object at 0x00000182EF9D1640>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever1 = vectorstore1.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain1=create_retrieval_chain(retriever1,document_chain)\n",
    "retrieval_chain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = retrieval_chain1.invoke({\n",
    "    \"input\":\"Just because we fight without rancor and without selfish object\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the main reasons given for entering the war according to the provided context?\\n\\nThe main reasons given for entering the war, according to the provided context, are to make the world safe for democracy, to establish peace on the foundations of political liberty, and to secure the rights of mankind. The speaker emphasizes that the intent is not based on selfish interests, territorial conquest, or material compensation. Instead, the focus is on fighting for democracy, the right of people to have a voice in their government, and the liberties of small nations. The goal is to create a universal dominion of right that will bring peace and safety to all nations, making the world free. There is a desire to act as sincere friends to the German people, opposing their government for discarding humanity and right, rather than acting out of enmity or seeking to harm. The effort is portrayed as a moral duty driven by principles, rather than animosity.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
